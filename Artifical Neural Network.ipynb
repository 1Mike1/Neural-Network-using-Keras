{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network (ANN)\n",
    "An artificial neural network is an attempt to simulate the network of neurons that make up a human brain so that the computer will be able to learn things and make decisions in a humanlike manner. ANNs are created by programming regular computers to behave as though they are interconnected brain cells.\n",
    "\n",
    "### Building Neural Network using Keras for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classifications\n",
    "Binary or binomial classification is the task of classifying the elements of a given set into two groups (predicting which group each one belongs to) on the basis of a classification rule.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Keras?\n",
    "Keras is a high-level neural network API which is written in Python and it wraps the efficient numerical computation libraries <b>Theano and TensorFlow</b> and allows you to define and train neural network models in just a few lines of code.\n",
    "\n",
    "\n",
    "Keras can be used as a deep learning library because it also support <b>Convolutional and Recurrent Neural Networks</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For binary classification we are using Bank Customers data.\n",
    "\n",
    "There are <b>10000 observations with 13 input variables and 1 output variable</b>.\n",
    "Using this dataset we will build a model which will predict whether the customer will leave the bank or not.\n",
    "\n",
    "### Below are the details of cutomer.\n",
    "\n",
    "#### Variables Name:\n",
    "1. RowNumber\n",
    "2. CustomerId\n",
    "3. Surname\n",
    "4. CreditScore\n",
    "5. Geography\n",
    "6. Gender\n",
    "7. Age(years)\n",
    "8. Tenure\n",
    "9. Balance\n",
    "10. NumOfProducts\n",
    "11. HasCrCard\n",
    "12. IsActiveMember\n",
    "13. EstimatedSalary\n",
    "14. Exited(Target variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "\n",
    "First import the basic libraries\n",
    "1. pandas.\n",
    "2. numpy.\n",
    "3. sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv('Customer_data.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe the dataset for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2932</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Smith</td>\n",
       "      <td>NaN</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5014</td>\n",
       "      <td>5457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569094e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>650.528800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.921800</td>\n",
       "      <td>5.012800</td>\n",
       "      <td>76485.889288</td>\n",
       "      <td>1.530200</td>\n",
       "      <td>0.70550</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>100090.239881</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.89568</td>\n",
       "      <td>7.193619e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.653299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.487806</td>\n",
       "      <td>2.892174</td>\n",
       "      <td>62397.405202</td>\n",
       "      <td>0.581654</td>\n",
       "      <td>0.45584</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>57510.492818</td>\n",
       "      <td>0.402769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.556570e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2500.75000</td>\n",
       "      <td>1.562853e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>584.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51002.110000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569074e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>652.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>97198.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100193.915000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7500.25000</td>\n",
       "      <td>1.575323e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>127644.240000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>149388.247500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.581569e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>250898.090000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>199992.480000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          RowNumber    CustomerId Surname   CreditScore Geography Gender  \\\n",
       "count   10000.00000  1.000000e+04   10000  10000.000000     10000  10000   \n",
       "unique          NaN           NaN    2932           NaN         3      2   \n",
       "top             NaN           NaN   Smith           NaN    France   Male   \n",
       "freq            NaN           NaN      32           NaN      5014   5457   \n",
       "mean     5000.50000  1.569094e+07     NaN    650.528800       NaN    NaN   \n",
       "std      2886.89568  7.193619e+04     NaN     96.653299       NaN    NaN   \n",
       "min         1.00000  1.556570e+07     NaN    350.000000       NaN    NaN   \n",
       "25%      2500.75000  1.562853e+07     NaN    584.000000       NaN    NaN   \n",
       "50%      5000.50000  1.569074e+07     NaN    652.000000       NaN    NaN   \n",
       "75%      7500.25000  1.575323e+07     NaN    718.000000       NaN    NaN   \n",
       "max     10000.00000  1.581569e+07     NaN    850.000000       NaN    NaN   \n",
       "\n",
       "                 Age        Tenure        Balance  NumOfProducts    HasCrCard  \\\n",
       "count   10000.000000  10000.000000   10000.000000   10000.000000  10000.00000   \n",
       "unique           NaN           NaN            NaN            NaN          NaN   \n",
       "top              NaN           NaN            NaN            NaN          NaN   \n",
       "freq             NaN           NaN            NaN            NaN          NaN   \n",
       "mean       38.921800      5.012800   76485.889288       1.530200      0.70550   \n",
       "std        10.487806      2.892174   62397.405202       0.581654      0.45584   \n",
       "min        18.000000      0.000000       0.000000       1.000000      0.00000   \n",
       "25%        32.000000      3.000000       0.000000       1.000000      0.00000   \n",
       "50%        37.000000      5.000000   97198.540000       1.000000      1.00000   \n",
       "75%        44.000000      7.000000  127644.240000       2.000000      1.00000   \n",
       "max        92.000000     10.000000  250898.090000       4.000000      1.00000   \n",
       "\n",
       "        IsActiveMember  EstimatedSalary        Exited  \n",
       "count     10000.000000     10000.000000  10000.000000  \n",
       "unique             NaN              NaN           NaN  \n",
       "top                NaN              NaN           NaN  \n",
       "freq               NaN              NaN           NaN  \n",
       "mean          0.515100    100090.239881      0.203700  \n",
       "std           0.499797     57510.492818      0.402769  \n",
       "min           0.000000        11.580000      0.000000  \n",
       "25%           0.000000     51002.110000      0.000000  \n",
       "50%           1.000000    100193.915000      0.000000  \n",
       "75%           1.000000    149388.247500      0.000000  \n",
       "max           1.000000    199992.480000      1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "As we can see that all features are not numerical and we do have categorical data. As we have categorical variables we need to do some data conversion of categorical variables and also we have to do some <b>feature engineering</b> on our dataset to optimize our model accuracy by providing best features to our model while training.\n",
    "\n",
    "\n",
    "### What is Feature Engineering?\n",
    "\n",
    "Feature engineering is a part of data pre-processing, where we will analyze or understand our dataset and try to find the strongest relationship of a feature with the target variable. If all feature has strong relationships with target then we don't do any update on our dataset else we will remove those columns from the dataset which are not impacting on the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "1. Here We can see that <b>RowNumber,CustomerId and Surname</b> are not impacting our target variable because the decision of a customer to leave the bank does not depend on the <b>customer surname and customerid</b>. So we can consider this as <b>weak features</b> in our dataset which will minimize the accuracy of our model.\n",
    "2. We can also see that <b>Geography</b> and <b>Gender</b> are categorical data so we need to do data conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the dataset on the above observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing CustomerId and Surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "# Features (Indepenndent Variable)\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "\n",
    "# Target (Dependent Varaiable)\n",
    "Y = dataset.iloc[:,-1].values\n",
    "\n",
    "# After Removing CustomerId and Surname\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 0 0 ... 1 1 101348.88]\n",
      " [608 2 0 ... 0 1 112542.58]\n",
      " [502 0 0 ... 1 0 113931.57]\n",
      " ...\n",
      " [709 0 0 ... 0 1 42085.58]\n",
      " [772 1 1 ... 1 0 92888.52]\n",
      " [792 0 0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "labelencoder_X1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X1.fit_transform(X[:, 1])\n",
    "\n",
    "labelencoder_X2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X1.fit_transform(X[:, 2])\n",
    "\n",
    "# After Data Conversion\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  \n",
    "### Preparing Dataset for training and testing.\n",
    "\n",
    "We now split the input features and target variables into training and test dataset. our test dataset will be 20% of our entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  \n",
    "<b>Since our input features are at different scales we need to standardize the input.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(X_train)\n",
    "x_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of keras\n",
    "\n",
    "We have preprocessed the data and we are now ready to build the neural network. Here We are using keras to build our neural network. \n",
    "\n",
    "So let's import the keras library to create our first neural network layers.\n",
    "\n",
    "To Create our neural network:-\n",
    "1. We will use Sequential model to build our neural network.\n",
    "2. We will use Dense library to build input, hidden and output layers of a neural network.    \n",
    "\n",
    "\n",
    "### Model Architecture\n",
    "1. <b>Layers</b><br>\n",
    "    We have <b>10 input features</b> and one target variable. <b>2 Hidden layers</b>. Each hidden layer will have <b>6 nodes.</b><br><br>\n",
    "\n",
    "2. <b>Activation Function</b><br>\n",
    "    <b>ReLu</b> will be the activation function for hidden layers. As this is a binary classification problem we will use <b>sigmoid as the activation function</b>.<br><br>\n",
    "\n",
    "3. <b>Loss Function</b><br>\n",
    "    As this is a binary classification problem, we use <b>binary_crossentropy</b> to calculate the loss function between the actual output and the predicted output.<br><br>\n",
    "\n",
    "4. <b>Optimizer</b><br>\n",
    "    To optimize our neural network we use <b>Adam</b>. Adam stands for Adaptive moment estimation. Adam is a combination of RMSProp + Momentum.\n",
    "        \n",
    "<b>we use accuracy as the metrics to measure the performance of the model.</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Representation Of Neural Network Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Neural_Net_Arch.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mithlesh.kumar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mithlesh.kumar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6)`\n",
      "  \n",
      "C:\\Users\\mithlesh.kumar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6)`\n",
      "  import sys\n",
      "C:\\Users\\mithlesh.kumar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(output_dim=6, activation='relu'))\n",
    "classifier.add(Dense(output_dim=6, activation='relu'))\n",
    "classifier.add(Dense(output_dim=1 ,activation='sigmoid'))\n",
    "\n",
    "classifier.compile(\n",
    "                    optimizer='adam',\n",
    "                    loss = 'binary_crossentropy',\n",
    "                    metrics = ['accuracy']\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Now we are going to fit our training data to the model we created. We use a batch_size of 10. \n",
    "This implies that we use 10 samples per gradient update.\n",
    "\n",
    "We iterate over 50 epochs to train the model. An epoch is an iteration over the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mithlesh.kumar\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.5286 - acc: 0.7627\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 0.4294 - acc: 0.8101\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 1s 105us/step - loss: 0.3985 - acc: 0.8125\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.3855 - acc: 0.8175\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 0.3788 - acc: 0.8260\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 0.3748 - acc: 0.8317\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 0.3726 - acc: 0.8336\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.3705 - acc: 0.8351\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 0.3682 - acc: 0.8376\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3655 - acc: 0.8410\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.3617 - acc: 0.8431\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3582 - acc: 0.8482\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.3550 - acc: 0.8535\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.3526 - acc: 0.8539\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.3503 - acc: 0.8529\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3489 - acc: 0.8545\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3478 - acc: 0.8566\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 0.3463 - acc: 0.8571\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 0.3459 - acc: 0.8580\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.3446 - acc: 0.8579\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.3445 - acc: 0.8577\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.3437 - acc: 0.8587\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3436 - acc: 0.8587\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.3430 - acc: 0.8587\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.3423 - acc: 0.8591\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.3421 - acc: 0.8605\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.3419 - acc: 0.8590\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 0.3415 - acc: 0.8595\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 0.3411 - acc: 0.8597\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3411 - acc: 0.8577\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.3407 - acc: 0.8584\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3405 - acc: 0.8600\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3402 - acc: 0.8589\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.3406 - acc: 0.8597\n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.3397 - acc: 0.8581\n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.3400 - acc: 0.8580\n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 0.3397 - acc: 0.8595\n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 0.3389 - acc: 0.8594\n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.3391 - acc: 0.8604\n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3386 - acc: 0.8605\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.3388 - acc: 0.8614\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 0.3386 - acc: 0.8594\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 0.3383 - acc: 0.8604\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.3383 - acc: 0.8607\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.3385 - acc: 0.8605\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 0.3381 - acc: 0.8624\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 0.3383 - acc: 0.8602\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 0.3384 - acc: 0.8606\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.3375 - acc: 0.8602\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.3378 - acc: 0.8609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2774ab796d8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x_train, y_train, batch_size=10, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see that the accuracy of our model is 86%  in 50 epocs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b>We can also evaluate the loss value & metrics values for the model in test mode using evaluate function</b></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 20us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3353754553794861, 0.86275]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model=classifier.evaluate(x_train, y_train)\n",
    "eval_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing\n",
    "\n",
    "Now we have trained model with us, so its time to test our model on test dataset.\n",
    "We now predict the output for our test dataset. If the prediction is greater than 0.5 then the output is 1(True) else the output is 0(False)\n",
    "\n",
    "If the output of our model is <b>True</b> then customer will close his/her account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]\n",
      " [False]\n",
      " [False]\n",
      " ...\n",
      " [False]\n",
      " [False]\n",
      " [False]]\n"
     ]
    }
   ],
   "source": [
    "y_pred=classifier.predict(x_test)\n",
    "result=y_pred>0.5\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Random Check\n",
    "new_pre = sc.transform(np.array([[519,0,0,32,1,0.1,0,1,1,23000]]))\n",
    "result = classifier.predict(new_pre)>0.5\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above are the output of our model. If you want to do some random test please update the values in random section accordingly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
